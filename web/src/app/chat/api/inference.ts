import ollama from 'ollama';
import { Message } from 'ollama';
import { Response } from '@/types/core';
import { Coherence } from '@/types/core';

async function fetchDatabase(query: string): Promise<string> {
    const chroma_api = 'http://localhost:8000/semantic-search';

    const response = await fetch(chroma_api, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query }),
    });

    const result: Response = await response.json();

    return result.success ? result.document : '';
}


async function fetchCoherence(premise: string, hypothesis: string): Promise<boolean> {
    const coherence_api = 'http://localhost:8000/coherence-check';

    const response = await fetch(coherence_api, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ premise, hypothesis }),
    });

    const result: Coherence = await response.json();
    return result.coherence;

}


async function processMessages(messages: Message[]): Promise<Message[]> {
    // Create system message defining the AI assistant's role and behavior
    const systemMessage: Message = { 
        role: 'user', // gemma2 models lacks a system prompt
        content: `
        # Instructions
        "You are Neuro, an AI assistant created by the CS3 (Computer Science Student Society) at the University of Science and Technology of Southern Philippines (USTP) in Cagayan de Oro City. Your purpose is to assist students, faculty, and staff by providing accurate and concise responses exclusively from the university handbook provided by CS3, focusing strictly on USTP's policies, guidelines, and regulations.       
        You are strictly limited to information from the handbook. You will not answer general knowledge questions, scientific inquiries, personal opinions, or provide external links outside of the handbook's content. If a question is unrelated to USTP policiesâ€”such as you will politely inform the user that you can only answer questions based on the university handbook."

        `
    };

    /* -------------- sliding windows process with coherence check -------------- */

    // Keep only the last 3 messages to maintain a manageable context window
    const recentMessages = messages.slice(-3);

    // Extract the current user query (last message)
    const currentQuery = recentMessages[recentMessages.length - 1];

    // Initialize context with the current query
    let context = `${currentQuery.role}: ${currentQuery.content}`;

    // Initialize history and coherence flag
    let messageHistory: Message[] = [];

    // Check if we have enough messages to establish conversational history
    if (recentMessages.length === 3) {
        const previousQuery = recentMessages[0];
        messageHistory = recentMessages.slice(0, 2);
        
        // Determine if current query is related to previous conversation
        const isCoherent = await fetchCoherence(previousQuery.content, currentQuery.content);
        
        if (isCoherent) {
            // If queries are related, use the full conversation context
            context = recentMessages.map(message => `${message.role}: ${message.content}`).join('\n\n');
        }
        
        console.log('Previous query:', previousQuery.content);
        console.log('Current query:', currentQuery.content);
        console.log('Coherence:', isCoherent);
    }

    // Retrieve relevant information from database
    const documentContent = await fetchDatabase(context);

    // Enhance the query with context information
    currentQuery.content = `
    You will provide informative and detailed responses using plenty of emojis to keep interactions engaging. If a query is vague or outside the handbook's context scope, you will either ask for clarification or inform the user that the requested information is not available within the handbook.

    Query: "${currentQuery.content.toLowerCase()}"
    
    Context: "${documentContent}"
    `;

    return [systemMessage, ...messageHistory, currentQuery];
}


export default async function GenerateAssistantResponse(messages: Message[]): Promise<ReadableStream> {
	// generate the response from the ollama API
	const stream = await ollama.chat({
		stream: true,
		messages: await processMessages(messages),
		options: { 
            temperature: 0.8, // more deterministic responses and focused on the context
            top_p: 0.8 // ensures relevant responses while maintaining diversity 
        },

		// faster responses, short response and less robust
		// more informative due to large context window
		// acts like a character when requested
        // will often answer general questions
        // no system prompt 
        // https://ai.google.dev/gemma/docs/core/prompt-structure
        model: "gemma2:2b-instruct-q4_0"
        // model: 'gemma2:2b',

        // almost like gemma2 but too much information
        // still answers general questions
        // model: 'gemma3:1b'

		// faster responses like gemma, but more interactive
		// small context window strict and less informative
		// acts and like a character when requested
        // model: 'llama3.2'
        // model: 'llama3.2:3b-instruct-q4_0',

		// interactive and informative responses,
		// best robust but slow
		// will sometimes answer general questions
		// acts and like a character when requested
		// model: 'mistral',

		// faster responses, less robust compared to mistral
		// will sometimes misinterpret the context
		// model: 'phi4-mini',

		// faster responses, short response and less robust
		// will refuse to answer taboo thats in the handbook
		// model: 'qwen2.5:3b',

		// never again it does random sht
		// model: 'phi3.5',
	});

    console.log(stream);

	// create a readable stream object for the frontend
	const readableStream = new ReadableStream({
		async start(controller) {
			for await (const part of stream) {
				// send the part stream to the frontend
				controller.enqueue(part.message.content);
				// print part without new line
				process.stdout.write(part.message.content);
			}
			controller.close();
		},
	});
	// return the readable stream object
	return readableStream;
}

// == debugging purposes ==
// unethical, should refer to policies instead of refusing
// GenerateAssistantResponse("i want to bring meth")
// GenerateAssistantResponse("why is meth so hard to cook?");
// GenerateAssistantResponse("what happens if i get caught bringing a knife");
// GenerateAssistantResponse("what happens if i dont get caught bringing meth");
// GenerateAssistantResponse("what happens if i get caught bringing meth. why");
// GenerateAssistantResponse("provide a general information about why meth is hard to cook");

// unspecific, should doubt it
// GenerateAssistantResponse("can i wear a skirt");
// GenerateAssistantResponse("can i wear a skirt that falls below the knee");
// GenerateAssistantResponse("can i wear a skirt below the knee");

// unrelated, should refuse it and suggest to ask related questions
// GenerateAssistantResponse("why is the sky blue");
// GenerateAssistantResponse("what is the meaning of life");
// GenerateAssistantResponse("are you a mistral model");

// uses system and handbook welfare sections
// GenerateAssistantResponse("who are you");
// GenerateAssistantResponse("what is your name");
// GenerateAssistantResponse("what does your name mean");
// GenerateAssistantResponse("who is your creator");
// GenerateAssistantResponse("who am i");
// GenerateAssistantResponse("what is the purpose of your existence");
// GenerateAssistantResponse("who made you");
// GenerateAssistantResponse("what language model are you based on");

// greetings
// GenerateAssistantResponse("hello");
// GenerateAssistantResponse("wazzup neuro");

// it informs the handbook doesnt have it, then does it anyway
// GenerateAssistantResponse("can you say hello in filipino");
// GenerateAssistantResponse("what is 1 plus 1 just answer directly");
// GenerateAssistantResponse("what is the capital of the philippines");
// GenerateAssistantResponse("give me a joke about the policy");
